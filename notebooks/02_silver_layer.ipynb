{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7900c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging\n",
    "from typing import Optional, Union\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from concurrent.futures import Future, as_completed\n",
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.errors as pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e423b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SILVER_PATH = '../data/silver/'\n",
    "BRONZE_PATH = '../data/bronze/'\n",
    "RAW_PATH = '../raw'\n",
    "LOG_DIR = '../log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02984f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"Silver_Layer_Log\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "log_path = os.path.join(LOG_DIR,\"silver.log\")\n",
    "fh = logging.FileHandler(log_path)\n",
    "sh = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "# sh.setFormatter(formatter)\n",
    "\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(fh)\n",
    "# logger.addHandler(sh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"Retail_ETL_Pipeline\").\\\n",
    "    config(\"spark.sql.shuffle.partitions\", \"4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909e5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read bronze files\n",
    "def load_bronze(table:str) -> DataFrame:\n",
    "    file_path = os.path.join(BRONZE_PATH,f\"{table}_clean.parquet\")\n",
    "    logger.info(f\"========== Loading {table} data from {file_path} ==========\")\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"No files found in {file_path}\")\n",
    "        df = spark.read.parquet(file_path)\n",
    "        logger.info(f\"Successfully loaded data from {file_path} to DataFrame\")\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        logger.exception(f\"Loading {table} data from {file_path} failed: {e}\", exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cea2fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean bronze files\n",
    "def extend_customers(df:DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean and extend customers\"\"\"\n",
    "    df = df.withColumnsRenamed({\"customer_fname\":\"first_name\", \n",
    "                                \"customer_lname\":\"last_name\",\n",
    "                                \"customer_street\":\"street\",\n",
    "                                \"customer_city\":\"city\",\n",
    "                                \"customer_state\":\"state\",\n",
    "                                \"customer_zipcode\":\"zipcode\"})\n",
    "    logger.debug(\"Renamed columns in customers for uniformity\")\n",
    "    df = df.withColumn('state',sf.ucase(\"state\")).\\\n",
    "        withColumn('full_name',sf.concat('first_name', sf.lit(' '), 'last_name'))\n",
    "    logger.debug(\"Added customer's full_name column\")\n",
    "    return df\n",
    "\n",
    "def extend_orders(df:DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean and extend orders table\"\"\"\n",
    "    # rename columns\n",
    "    df = df.withColumnRenamed(\"order_customer_id\",\"customer_id\")\n",
    "    logger.debug(\"Renamed columns in orders for uniformity\")\n",
    "    df = df.withColumns({\n",
    "        \"order_year\":sf.year(\"order_date\"),\n",
    "        \"order_month\":sf.month(\"order_date\"),\n",
    "        # \"order_monthname\":sf.monthname(\"order_date\"), - abbreviated\n",
    "        \"order_monthname\":sf.date_format(\"order_date\",\"MMMM\"),\n",
    "        \"order_day\":sf.day(\"order_date\"),\n",
    "        # \"order_weekday\":sf.dayname(\"order_date\") - abbreviated\n",
    "        \"order_weekday\":sf.date_format(\"order_date\",\"EEEE\")})\n",
    "    logger.debug(\"Added columns for year, month, day, and weekday from order_date timestamp\")\n",
    "    return df\n",
    "\n",
    "def extend_order_items(df:DataFrame) -> DataFrame:\n",
    "    # Rename Columns\n",
    "    df = df.withColumnsRenamed({\n",
    "        \"order_item_order_id\":\"order_id\",\n",
    "        \"order_item_product_id\":\"product_id\",\n",
    "        \"order_item_quantity\":\"quantity\",\n",
    "        \"order_item_subtotal\":\"subtotal\",\n",
    "        \"order_item_product_price\":\"product_price\",\n",
    "        })\n",
    "    logger.debug(\"Renamed columns in order_items for uniformity\")\n",
    "    # Validated the subtotal logic \n",
    "    df.createOrReplaceTempView(\"temp_oi_logic_check\")\n",
    "    df_count = spark.sql(\"\"\"\n",
    "            SELECT COUNT(1) num_invalid \n",
    "            FROM temp_oi_logic_check\n",
    "            WHERE ROUND(quantity * product_price,2) <> subtotal\"\"\")\n",
    "    logger.debug(\"Validating the order items subtotal logic\")\n",
    "    count = df_count.first()\n",
    "    logger.warning(f\"Number of records where subtotal calculation is invalid: {count.num_invalid}\")\n",
    "    if count.num_invalid > 0:\n",
    "        df = df.withColumn('subtotal',sf.round(df.quantity * df.product_price,2))\n",
    "        logger.debug(\"Replaced subtotal values: quantity * product price\")\n",
    "    return df\n",
    "\n",
    "def extend_categories(df:DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean categories table\"\"\"\n",
    "    df = df.withColumnRenamed(\"category_department_id\",\"department_id\")\n",
    "    logger.debug(\"Renamed columns in categories for uniformity\")\n",
    "    return df\n",
    "\n",
    "def extend_products(df:DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean products table\"\"\"\n",
    "    df = df.withColumnsRenamed({\"product_cateogry_id\":\"category_id\",\n",
    "                                \"product_price\":\"price\"})\n",
    "    logger.debug(\"Renamed columns in Products for uniformity\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed93d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_dim(orders:DataFrame) -> DataFrame:\n",
    "    \"\"\"Create date dimension table\"\"\"\n",
    "    orders.createOrReplaceTempView(\"temp_orders\")\n",
    "    df = spark.sql(\"\"\"\n",
    "    SELECT EXPLODE(\n",
    "        SEQUENCE(\n",
    "        CAST(MIN(order_date) AS date),\n",
    "        CAST(MAX(order_date) AS date),\n",
    "        INTERVAL 1 DAY)) AS date_seq\n",
    "    FROM temp_orders\n",
    "    \"\"\")\n",
    "    logger.debug(\"Generate series of dates from min to max order_dates\")\n",
    "    df = df.withColumns({\n",
    "        'date_id': sf.date_format('date_seq','yyyyMMdd'),\n",
    "        'year': sf.year('date_seq'),\n",
    "        'quarter': sf.quarter('date_seq'),\n",
    "        'month': sf.month('date_seq'),\n",
    "        'month_name': sf.date_format('date_seq','MMMM'),\n",
    "        'day': sf.day('date_seq'),\n",
    "        'day_of_week': sf.weekday('date_seq'),\n",
    "        'day_name': sf.date_format('date_seq','EEEE'),\n",
    "    })\n",
    "    logger.debug(\"Added columns for year, quarter, month, and date\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fc864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sales_fact(order_items:DataFrame, orders:DataFrame, \\\n",
    "                   products:DataFrame, categories:DataFrame) -> DataFrame:\n",
    "    columns = [\"order_id\",\"order_item_id\",\"customer_id\",\"product_id\",\"category_id\",\\\n",
    "               \"department_id\",\"quantity\",\"subtotal\",\"product_price\",\"order_date\"]\n",
    "    try:\n",
    "        df = order_items.join(orders, on='order_id', how=\"left\")\n",
    "        logger.debug(\"Left joined order_items and orders\")\n",
    "        df = df.join(products, on='product_id', how=\"left\").\\\n",
    "            join(categories, on='category_id', how=\"left\")\n",
    "        logger.debug(\"Left joined products and categories\")\n",
    "        sales_fact = df.select(columns).sort(df.order_id.asc(),df.order_item_id.asc()).\\\n",
    "            withColumn('date_id',sf.date_format('order_date',\"yyyyMMdd\"))\n",
    "        logger.info(\"Successfully built sales_fact table\")\n",
    "        return sales_fact\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"An error occured while building sales_fact table: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_silver(df:DataFrame, name:str):\n",
    "    logger.info(f\"============ Saving {name} as Parquet ==========\")\n",
    "    try:\n",
    "        # create save path\n",
    "        save_path = os.path.join(SILVER_PATH,f\"{name}.parquet\")\n",
    "        logger.debug(f\"Created save path for {name}: '{save_path}'\")\n",
    "        # write to parquet\n",
    "        df.write.parquet(path=save_path,mode='overwrite')\n",
    "        logger.info(f\"Successfully saved {name} to '{save_path}'\")\n",
    "    except (pe.PySparkException, OSError) as e:\n",
    "        logger.error(f\"Error saving {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_silver_layer(table:str):\n",
    "    logger.info(f\"========== Started processing {table} ==========\")\n",
    "    try:\n",
    "        df = load_bronze(table)\n",
    "        silver_fn_map = {\n",
    "            \"orders\": extend_orders,\n",
    "            \"order_items\": extend_order_items,\n",
    "            \"products\": extend_products,\n",
    "            \"categories\": extend_categories,\n",
    "            \"customers\": extend_customers,\n",
    "        }\n",
    "        cleaned = silver_fn_map.get(table,lambda x: x)(df)\n",
    "        save_as_silver(cleaned,table)\n",
    "        logger.info(f\"========== Finished processing {table} ==========\")\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error processing {table}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c0382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac712017",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = extend_order_items(load_bronze('order_items'))\n",
    "    orders = extend_orders(load_bronze('orders'))\n",
    "    products = extend_products(load_bronze('products'))\n",
    "    categories = extend_categories(load_bronze('categories'))\n",
    "\n",
    "    save_as_silver(get_sales_fact(df, orders,products,categories),\"sales_fact\")\n",
    "    save_as_silver(get_date_dim(orders),\"date_dim\")\n",
    "except Exception as e:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb06a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id', 'order_date', 'order_customer_id', 'order_status']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enrich tables\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as silver.parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
