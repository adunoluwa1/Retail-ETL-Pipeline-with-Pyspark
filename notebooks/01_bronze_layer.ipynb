{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc9033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.errors as pe\n",
    "from typing import Optional\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bc0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path = 'raw/schemas.json'\n",
    "bronze_path = 'data/bronze'\n",
    "raw_path = 'raw'\n",
    "log_path = 'log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113b602",
   "metadata": {},
   "source": [
    "Set Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646d53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "os.makedirs(log_path,exist_ok=True)\n",
    "log_file_path = os.path.join(log_path,'application.log')\n",
    "fh = logging.FileHandler(log_file_path)\n",
    "# sh = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "# sh.setFormatter(formatter)\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(fh)\n",
    "# logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdcaae",
   "metadata": {},
   "source": [
    "Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a198be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/04 12:38:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Retail_ETL_Pipeline\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\").getOrCreate()\n",
    "logger.info(\"Initialized SparkSession Retail_ETL_Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33ad28",
   "metadata": {},
   "source": [
    "Load schema.json to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5c3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract schema as dict\n",
    "def get_schema_json(path:str='**/schemas.json') -> dict:\n",
    "    # find schema path\n",
    "    schema_path = glob.glob(pathname=path, recursive=True)\n",
    "    try:\n",
    "        # if no schema_path is found\n",
    "        if len(schema_path) == 0:\n",
    "            raise FileNotFoundError(f\"Schema not found in {path}\")\n",
    "        # if multiple schema paths are found\n",
    "        if len(schema_path) > 1:\n",
    "            raise FileNotFoundError(f\"Multiple schema files found in {path}\")\n",
    "        schema_path = schema_path[0] \n",
    "        # read schema from path and parse as dict\n",
    "        with open(schema_path,'r') as file:\n",
    "            schema_dict = json.load(file)\n",
    "            logger.info(f\"Successfully parsed schema '{schema_path}' as dict\")\n",
    "        return schema_dict\n",
    "    except FileNotFoundError as e:\n",
    "        logger.exception(f\"Unable to load schema: {e}\", exc_info=True)\n",
    "        raise e\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.exception(f\"Unable to parse json: {e}\", exc_info=True)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feeb958",
   "metadata": {},
   "source": [
    "extract dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7835048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from schema\n",
    "def get_dataframe_schema(schema_dict:dict[str,list],table:str,sorting_key:str='column_position') -> StructType:\n",
    "    table_schema:Optional[list[dict]] = schema_dict.get(table,None)\n",
    "    if not table_schema:\n",
    "        logger.error(\"Table not found in schema\")\n",
    "        return StructType([])\n",
    "    logger.info(f\"========== Parsing {table} ========== \")\n",
    "    columns = []\n",
    "    try:\n",
    "        for i, col in enumerate(sorted(table_schema, key=lambda x: x[sorting_key])):\n",
    "            col_name = col.get(\"column_name\",f\"unknown_col_{i}\")\n",
    "            data_type_name = f\"{col.get('data_type','string').title()}Type\"\n",
    "            data_type_class = globals().get(data_type_name,'StringType')\n",
    "            columns.append(StructField(col_name, data_type_class(),nullable=True))\n",
    "            logger.info(f\"Successfully parsed column {i + 1}: {col_name} of {len(table_schema)}\")\n",
    "        return StructType(columns)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error parsing col: {col_name},{data_type_name}\", exc_info=True)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3410b",
   "metadata": {},
   "source": [
    "load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(src_path:str, table:str,schema:StructType,pattern:str=\"part-*\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    table_dir_path = os.path.join(src_path,table) \n",
    "    search_path = os.path.join(table_dir_path,pattern) \n",
    "    path_list = glob.glob(pathname=search_path,recursive=True)\n",
    "    try:\n",
    "        # raise FileNotFoundError no file exists \n",
    "        if len(path_list) == 0:\n",
    "            raise FileNotFoundError(f\"File not found in {search_path}\")\n",
    "        \n",
    "        # read files using Spark\n",
    "        df = spark.read.csv(table_dir_path,schema=schema,sep=',')\n",
    "        \n",
    "        # calculate number of records\n",
    "        # df.createOrReplaceTempView(f\"temp_{table}\")\n",
    "        # df_count = spark.sql(f\"SELECT COUNT(*) AS num_records FROM temp_{table}\")\n",
    "        # count = df_count.first()\n",
    "        \n",
    "        # logger.info(f\"Successfuly parsed {count.num_records} records from directory: {table_dir_path}\")\n",
    "        logger.info(f\"Successfuly loaded {table} from directory: {table_dir_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        logger.exception(f\"Error loading data, {e}\", exc_info=True)\n",
    "        raise e\n",
    "    except pe.ParseException as e:\n",
    "        logger.exception(f\"Error parsing {table_dir_path}: {e}\", exc_info=True)\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"An unhandled exception occurred: {e}\", exc_info=True)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6727d8",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8052b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customers(df:DataFrame) -> DataFrame:\n",
    "    logger.info(\"========== Cleaning customers data ==========\")\n",
    "    \n",
    "    # Calculate null counts and metrics via sql query\n",
    "    df.createOrReplaceTempView('temp_customers')\n",
    "    counts_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) AS id_count,\n",
    "        SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS null_count\n",
    "    FROM temp_customers \"\"\")\n",
    "    counts = counts_df.first()\n",
    "\n",
    "    # Log counts of unique ids\n",
    "    logger.info(f\"Record count of customers before cleaning: {counts.id_count}\")\n",
    "    logger.warning(f\"Record count of null values in primary key before cleaning: {counts.null_count}\")\n",
    "    \n",
    "    # Drop nulls and duplicate IDs\n",
    "    df = df.dropna(subset=['customer_id']).dropDuplicates(subset=['customer_id'])\n",
    "    logger.debug(\"Dropped nulls and duplicate customer ids\")\n",
    "    # Dropping email and password PII\n",
    "    columns = [col for col in df.columns if col not in ['customer_email','customer_password']]\n",
    "    df_bronze = df.select(columns)\n",
    "    logger.debug(\"Dropped customer email and password columns as PII\")\n",
    "\n",
    "    # Log total number of records after bronze cleaning\n",
    "    logger.info(f\"Record count of customers after bronze cleaning: {df_bronze.count()}\")\n",
    "    return df_bronze\n",
    "\n",
    "def clean_departments(df:DataFrame) -> DataFrame:\n",
    "    logger.info(\"========== Cleaning departments data ==========\")\n",
    "        \n",
    "    # Calculate null counts and metrics via sql query\n",
    "    df.createOrReplaceTempView('temp_departments')\n",
    "    counts_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) AS id_count,\n",
    "        SUM(CASE WHEN department_id IS NULL THEN 1 ELSE 0 END) AS null_count\n",
    "    FROM temp_departments \"\"\")\n",
    "    counts = counts_df.first()\n",
    "\n",
    "    # Log counts of unique ids\n",
    "    logger.info(f\"Record count of departments before cleaning: {counts.id_count}\")\n",
    "    logger.warning(f\"Record count of null values in primary key before cleaning: {counts.null_count}\")\n",
    "\n",
    "    # Drop nulls and duplicate IDs\n",
    "    df = df.dropna(subset=['department_id']).dropDuplicates(subset=['department_id'])\n",
    "    logger.debug(\"Dropped nulls and duplicate department ids\")\n",
    "    # Standardizing text in column\n",
    "    df_bronze = df.withColumn(\"department_name\",sf.trim(sf.initcap(df.department_name)))\n",
    "    logger.debug(\"Trimmed and capitalized the department name for uniformity\")\n",
    "\n",
    "    # Log total number of records after bronze cleaning\n",
    "    logger.info(f\"Record count of departments after bronze cleaning: {df_bronze.count()}\")\n",
    "    return df_bronze\n",
    "\n",
    "def clean_categories(df:DataFrame) -> DataFrame:\n",
    "    logger.info(\"========== Cleaning categories data ==========\")\n",
    "\n",
    "    # Calculate null counts and metrics via sql query\n",
    "    df.createOrReplaceTempView('temp_categories')\n",
    "    counts_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) AS id_count,\n",
    "        SUM(CASE WHEN category_id IS NULL THEN 1 ELSE 0 END) AS null_count\n",
    "    FROM temp_categories \"\"\")\n",
    "    counts = counts_df.first()\n",
    "\n",
    "    # Log counts of unique ids\n",
    "    logger.info(f\"Record count of categories before cleaning: {counts.id_count}\")\n",
    "    logger.warning(f\"Record count of null values in primary key before cleaning: {counts.null_count}\")\n",
    "\n",
    "    # Drop nulls and duplicate IDs\n",
    "    df = df.dropna(subset=['category_id']).dropDuplicates(subset=['category_id'])\n",
    "    logger.debug(\"Dropped nulls and duplicate category_ids\")\n",
    "    # Standardizing text in column\n",
    "    df_bronze = df.withColumn(\"category_name\",sf.trim(sf.initcap(df.category_name)))\n",
    "    logger.debug(\"Trimmed and capitalized the category name for uniformity\")\n",
    "\n",
    "    # Log total number of records after bronze cleaning\n",
    "    logger.info(f\"Record count of categories after bronze cleaning: {df_bronze.count()}\")\n",
    "    return df_bronze\n",
    "\n",
    "def clean_order_items(df:DataFrame) -> DataFrame:\n",
    "    logger.info(\"========== Cleaning order_items data ==========\")\n",
    "\n",
    "    # Calculate null counts and metrics via sql query\n",
    "    df.createOrReplaceTempView('temp_order_items')\n",
    "    counts_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) AS id_count,\n",
    "        SUM(CASE WHEN order_item_id IS NULL THEN 1 ELSE 0 END) AS null_count\n",
    "    FROM temp_order_items \"\"\")\n",
    "    counts = counts_df.first()\n",
    "\n",
    "    # Log counts of unique ids\n",
    "    logger.info(f\"Record count of order_items before cleaning: {counts.id_count}\")\n",
    "    logger.warning(f\"Record count of null values in primary key before cleaning: {counts.null_count}\")\n",
    "\n",
    "    # Drop nulls and duplicate IDs\n",
    "    df = df.dropna(subset=['order_item_id']).dropDuplicates(subset=['order_item_id'])\n",
    "    logger.debug(\"Dropped nulls and duplicate order_items ids\")\n",
    "    # converting data types from float to double for precision\n",
    "    df_bronze = df.withColumns({'order_item_subtotal':sf.round(sf.col('order_item_subtotal').cast(DoubleType()),2),\\\n",
    "                                'order_item_product_price':sf.round(sf.col('order_item_product_price').cast(DoubleType()),2)})\n",
    "    logger.debug(\"Converted float data types to double for greater precision\")\n",
    "\n",
    "    # Log total number of records after bronze cleaning\n",
    "    logger.info(f\"Record count of order_items table after bronze cleaning: {df_bronze.count()}\")\n",
    "    return df_bronze\n",
    "\n",
    "def clean_orders(df:DataFrame) -> DataFrame:\n",
    "    logger.info(\"========== Cleaning orders data ==========\")\n",
    "\n",
    "    # Calculate null counts and metrics via sql query\n",
    "    df.createOrReplaceTempView('temp_orders')\n",
    "    counts_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) AS id_count,\n",
    "        SUM(CASE WHEN order_id IS NULL THEN 1 ELSE 0 END) AS null_count\n",
    "    FROM temp_orders \"\"\")\n",
    "    counts = counts_df.first()   \n",
    "    \n",
    "    # Log counts of unique ids\n",
    "    logger.info(f\"Record count of orders before cleaning: {counts.id_count}\")\n",
    "    logger.warning(f\"Record count of null values in primary key before cleaning: {counts.null_count}\")\n",
    "\n",
    "    # Drop nulls and duplicate IDs\n",
    "    df_bronze = df.dropna(subset=['order_id']).dropDuplicates(subset=['order_id'])\n",
    "    logger.debug(\"Dropped nulls and duplicate order ids\")\n",
    "\n",
    "    # Log total number of records after bronze cleaning\n",
    "    logger.info(f\"Record count of orders table after bronze cleaning: {df_bronze.count()}\")\n",
    "    return df_bronze\n",
    "\n",
    "def clean_products(df:DataFrame) -> DataFrame:\n",
    "    logger.info(\"========== Cleaning products data ==========\")\n",
    "\n",
    "    # Calculate null counts and metrics via sql query\n",
    "    df.createOrReplaceTempView('temp_products')\n",
    "    counts_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) AS id_count,\n",
    "        SUM(CASE WHEN product_id IS NULL THEN 1 ELSE 0 END) AS null_count\n",
    "    FROM temp_products \"\"\")\n",
    "    counts = counts_df.first() \n",
    "    \n",
    "    # Log counts of unique ids\n",
    "    logger.info(f\"Record count of products before cleaning: {counts.id_count}\")\n",
    "    logger.warning(f\"Record count of null values in primary key before cleaning: {counts.null_count}\")\n",
    "    \n",
    "    # removing unnecessary columns\n",
    "    columns = [col for col in df.columns if col not in ['product_description','product_image']]\n",
    "    logger.debug(\"Dropped unnecessary columns - product_description and product_image\")\n",
    "    # Drop nulls and duplicate IDs\n",
    "    df = df.select(columns).dropna(subset=['product_id']).dropDuplicates(subset=['product_id'])\n",
    "    logger.debug(\"Dropped nulls and duplicate product ids\")\n",
    "    # converting data types from float to double for precision\n",
    "    df_bronze = df.withColumn(\"product_price\",sf.round(df.product_price.cast(DoubleType()),2))\n",
    "    logger.debug(\"Converted float data types to double for greater precision\")\n",
    "\n",
    "    # Log total number of records after bronze cleaning\n",
    "    logger.info(f\"Record count of products table after bronze cleaning: {df_bronze.count()}\")\n",
    "    return df_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd517718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "def save_as_bronze(df:DataFrame, name:str):\n",
    "    logger.info(f\"============ Saving {name} as Parquet ==========\")\n",
    "    try:\n",
    "        # create save path\n",
    "        save_path = os.path.join(bronze_path,f\"{name}_clean.parquet\")\n",
    "        logger.debug(f\"Created save path for {name}: '{save_path}'\")\n",
    "        # write to parquet\n",
    "        df.write.parquet(path=save_path,mode='overwrite')\n",
    "        logger.info(f\"Successfully saved {name} to '{save_path}'\")\n",
    "    except (pe.PySparkException, OSError) as e:\n",
    "        logger.error(f\"Error saving {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63797661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_bronze_table(name:str):\n",
    "    logger.info(f\"========== Started processing {name} ==========\")\n",
    "    try:\n",
    "        df = load_data(raw_path, name, get_dataframe_schema(schema_dict,name))\n",
    "        table_fn_map = {\n",
    "            'customers': clean_customers,\n",
    "            'departments': clean_departments,\n",
    "            'categories': clean_categories,\n",
    "            'order_items': clean_order_items,\n",
    "            'orders': clean_orders,\n",
    "            'products': clean_products,\n",
    "        }\n",
    "        cleaned = table_fn_map[name](df)\n",
    "        save_as_bronze(cleaned,name)\n",
    "        logger.info(f\"Finished processing {name}\")\n",
    "        return name\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error processing {name}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "schema_dict = get_schema_json(path=schema_path)\n",
    "tables = schema_dict.keys()\n",
    "\n",
    "# Use a thread pool to parallelize Spark jobs from the same SparkSession\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = {executor.submit(_process_bronze_table, t):t for t in tables}\n",
    "    for fut in as_completed(futures):\n",
    "        tbl = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Table {tbl} failed: {e}\")\n",
    "            raise            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465eee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
