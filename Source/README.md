# ğŸ›’ Retail ETL with PySpark 

This project implements a **real-world style ETL pipeline** for a retail dataset (customers, products, and orders).  
It follows the **medallion architecture** (Bronze â†’ Silver â†’ Gold) with industry-standard optimizations and logging practices.

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ input/ # Raw CSVs (ingestion)
â”‚ â”œâ”€â”€ bronze/ # Cleaned parquet
â”‚ â”œâ”€â”€ silver/ # Enriched & partitioned data
â”‚ â””â”€â”€ gold/ # Presentation KPIs
â”œâ”€â”€ logs/ # ETL log files
â”œâ”€â”€ notebooks/ # Jupyter notebooks (ingestion, transformation, gold KPIs)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md


## âš™ï¸ Pipeline Overview

### ğŸ”¹ Bronze Layer
- Ingest raw `orders`, `customers`, and `products`
- Store as **parquet**
- Add ingestion metadata (timestamps, source)

### ğŸ”¹ Silver Layer
- Join orders with customers & products  
- Compute `order_value`  
- Partition by `region`, `Year`, `Month`  
- Optimize small file writes (`coalesce`, `repartition`)  

### ğŸ”¹ Gold Layer
- Customer KPIs â†’ Lifetime Value, Avg Order Value, Active Months  
- Product KPIs â†’ Revenue, Units Sold, Avg Transaction Value  
- Regional KPIs â†’ Revenue & Units by Month  

---

