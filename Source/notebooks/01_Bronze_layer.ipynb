{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068f235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef38d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark & Logging Setup\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL-Pipeline\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "RAW_PATH = \"data/input\"\n",
    "BRONZE_PATH = \"data/bronze\"\n",
    "LOG_PATH = \"logs\"\n",
    "\n",
    "os.makedirs(f\"{BRONZE_PATH}\", exist_ok=True)\n",
    "os.makedirs(f\"{LOG_PATH}\", exist_ok=True)\n",
    "\n",
    "# Configure Logging\n",
    "\n",
    "logfile = os.path.join(LOG_PATH, \"etl_transformation_log.log\")\n",
    "logger = logging.getLogger(\"ETL\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "fh = logging.FileHandler(logfile)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# Avoid duplicate logs if handler already exists\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8a8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"=== Transformation job started ===\")\n",
    "\n",
    "def load_raw(name):\n",
    "    path = os.path.join(RAW_PATH, f\"{name}.csv\" )\n",
    "    logger.info(f\"loading raw file: {path}\")\n",
    "    return spark.read.option(\"header\", True).csv(path=path)\n",
    "\n",
    "def clean_orders(df):\n",
    "    logger.info(\"Cleaning orders data\")\n",
    "    df1 = df.dropDuplicates([\"order_id\"])\\\n",
    "            .withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\\\n",
    "            .dropna(subset = [\"order_date\"])\n",
    "    return df1\n",
    "\n",
    "def clean_products(df):\n",
    "    \"\"\"Clean products dataset\"\"\"\n",
    "    logger.info(\"Cleaning products data\")\n",
    "    return (df.dropDuplicates([\"product_id\"])\n",
    "              .withColumn(\"price\", col(\"price\").cast(\"double\")))\n",
    "\n",
    "def clean_customers(df):\n",
    "    \"\"\"Clean customers dataset\"\"\"\n",
    "    logger.info(\"Cleaning customers data\")\n",
    "    return df.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "def save_to_bronze(df, name):\n",
    "    path = os.path.join(BRONZE_PATH, f\"{name}.parquet\")\n",
    "    df.write.mode(\"overwrite\").parquet(path)\n",
    "    logger.info(f\"Saved {name} to Bronze layer: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "473fdaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Pipeline\n",
    "# ------------------\n",
    "try:\n",
    "    orders = load_raw(\"orders\")\n",
    "    products = load_raw(\"products\")\n",
    "    customers = load_raw(\"customers\")\n",
    "\n",
    "    orders_clean = clean_orders(orders)\n",
    "    products_clean = clean_products(products)\n",
    "    customers_clean = clean_customers(customers)\n",
    "\n",
    "    save_to_bronze(orders_clean, \"orders_clean\")\n",
    "    save_to_bronze(products_clean, \"products_clean\")\n",
    "    save_to_bronze(customers_clean, \"customers_clean\")\n",
    "\n",
    "    logger.info(\"Transformation job completed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Transformation job failed: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
